{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NetShare (Practical GAN-based Synthetic IP Header Trace Generation)\n",
        "\n",
        "NetShare is an end-to-end framework that utilizes Generative Adversarial Networks (GANs) to automatically generate synthetic packet and flow header traces for various networking purposes such as telemetry, anomaly detection, and provisioning."
      ],
      "metadata": {
        "id": "2anSLp7PEnMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up\n",
        "#### Step1. Clone the repository\n",
        "```\n",
        "git clone https://github.com/netsharecmu/NetShare_Summer2023_Internship\n",
        "```\n",
        "#### Step2. Set up virtual environment\n",
        "```\n",
        "python3.9 -m venv venv\n",
        "source venv/bin/activate\n",
        "```\n",
        "#### Step3. Install NetShare/SDMetrics packages\n",
        "```\n",
        "pip install -e NetShare/\n",
        "pip install -e SDMetrics_timeseries/\n",
        "```"
      ],
      "metadata": {
        "id": "oQ1VVPYRHxvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started\n",
        "\n"
      ],
      "metadata": {
        "id": "4nR0MrypNiVX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Data\n",
        "NetShare supports the following file formats,\n",
        "* *.csv\n",
        "* *.pcap\n",
        "* *.log"
      ],
      "metadata": {
        "id": "TY8kd0wiP72x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install (optional)\n",
        "If you're using Zeek's file, refer to this [link](https://colab.research.google.com/drive/1hYDGb_wO7N5lL8iOLlO2_DSzCpa4nRS9#scrollTo=02MLMk2sX-Q9&line=2&uniqifier=1) for additional install requirements."
      ],
      "metadata": {
        "id": "02MLMk2sX-Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Configuration File\n",
        "Create a configuration file in `JSON` format. Follow the steps below, using syslogs configuration file for reference."
      ],
      "metadata": {
        "id": "Vv4ZhBrQkLBx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step1. Config *processors*\n",
        "Config `\"processors\"` when need specific preprocessors and the postprocessor for raw data, leave it empty if not.\n",
        "\n",
        "##### Example1. Need specific processor for syslog dataset\n",
        "```\n",
        "  \"processors\": {\n",
        "    \"preprocessors\": [\n",
        "      {\n",
        "        \"class\": \"ZeekPreprocessor\",\n",
        "        \"config\": {\n",
        "          \"target_protocol\": \"syslog\"\n",
        "        }\n",
        "      },\n",
        "      {\n",
        "        \"class\": \"CustomizableFormatPreprocessor\",\n",
        "        \"config\": {\n",
        "          \"input_file_format\": \"zeek_log_json\"\n",
        "        }\n",
        "      },\n",
        "      {\n",
        "        \"class\": \"csv_pre_processor\"\n",
        "      }\n",
        "    ],\n",
        "    \"postprocessors\": [\n",
        "      {\n",
        "        \"class\": \"csv_post_processor\"\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "```\n",
        "\n",
        "##### Example2. Need no processor for raw data\n",
        "```\n",
        "  \"processors\": {\n",
        "    \"preprocessors\": [\n",
        "    ],\n",
        "    \"postprocessors\": [\n",
        "    ]\n",
        "  },\n",
        "```\n",
        "Supported Processors\n",
        "* preprocessors:\n",
        "    * `ZeekPreprocessor`: capable of calling `Zeek` to convert `PCAP` file to `Zeek` `.log` format.\n",
        "        * `target_protocol`: protocol of pcap/log file (eg. `syslog` and `modbus`)\n",
        "    * `CustomizableFormatPreprocessor`: capable of converting raw file in different file formats (e.g., `Zeek` `.log` format) into `CSV` file. Also capable of parsing each field respectively according to corresponding parse functions (allow customize).\n",
        "       * `input_file_format`: set to `csv` for .csv input file, set to `zeek_log_json` for .pcap or .log input file.\n",
        "    * `csv_pre_processor`: handle the data format that is incompatible with NetShare like IP address, timestamp and list format dataset.\n",
        "* postprocessorsï¼š\n",
        "    * `csv_post_processor`: convert the data format back to original format, must required if having \"csv_pre_processor\".\n",
        "\n"
      ],
      "metadata": {
        "id": "LRelIjdQkR4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step2. Config *global*\n",
        "```\n",
        "  \"global_config\": {\n",
        "    \"overwrite\": true,\n",
        "    \"dataset_type\": \"netflow\",\n",
        "    \"n_chunks\": 1,\n",
        "    \"dp\": false\n",
        "  },\n",
        "```\n",
        "* overwrite: default is `false`, change to `true` to overwrite existing working directory.\n",
        "* dataset_type: default is `netflow`, change to `pcap` if need to convert pcap to csv file.\n",
        "* n_chunks: number of valid chunks, set to 1 will reduce to plain DoppelGANger.\n",
        "* dp: default is `false`, set to `true` to enable differentially-private training."
      ],
      "metadata": {
        "id": "bdLnpIe5kXYX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step3. Config *default*\n",
        "```\n",
        "\"default\": \"single_event_per_row.json\"\n",
        "```\n",
        "* default: change to `dg_table_row_per_sample.json` for multi-event."
      ],
      "metadata": {
        "id": "PYoAttJzkY7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step4. Config *pre_post_processor*\n",
        "```\n",
        "\"pre_post_processor\": {\n",
        "    \"class\": \"NetsharePrePostProcessor\",\n",
        "    \"config\": {\n",
        "      \"word2vec\": {\n",
        "        \"vec_size\": 10,\n",
        "        \"model_name\": \"word2vec_vecSize\",\n",
        "        \"annoy_n_trees\": 100,\n",
        "        \"pretrain_model_path\": null\n",
        "      },\n",
        "      \"metadata\": [],\n",
        "      \"timeseries\": []\n",
        "    }\n",
        "  },\n",
        "  ```\n",
        "  Add to the config file, keep the defult configuration setting."
      ],
      "metadata": {
        "id": "pw4kb-UqLc-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step5. Config *model*\n",
        "```\n",
        "\"model\": {\n",
        "        \"class\": \"DoppelGANgerTorchModel\",\n",
        "        \"config\": {\n",
        "            \"batch_size\": 100,\n",
        "            \"sample_len\": [\n",
        "                10\n",
        "            ],\n",
        "            \"sample_len_expand\": true,\n",
        "            \"epochs\": 40,\n",
        "            \"extra_checkpoint_freq\": 1,\n",
        "            \"epoch_checkpoint_freq\": 5\n",
        "        }\n",
        "}\n",
        "    \n",
        "```\n",
        "* batch_size: number of samples used in each iteration of the training process.\n",
        "* sample_len: length of the input data samples model will generate.\n",
        "* sample_len_expand: set to `true` when generated samples will be expanded.\n",
        "* epochs:  number of epochs.\n",
        "* extra_checkpoint_freq: frequency for checkpoints saved during training."
      ],
      "metadata": {
        "id": "8uCTrdIRkfTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step6. Config *fields*\n",
        "Add timestamp, metadata, timeseries fields.\n",
        "```\n",
        " \"fields\": {\n",
        "    \"timestamp\": [\n",
        "    ],\n",
        "    \"metadata\": [\n",
        "    ],\n",
        "    \"timeseries\": [  \n",
        "    ]\n",
        "  }\n",
        "```\n",
        "Example for a field:\n",
        "```\n",
        "      {\n",
        "        \"name\": \"id.resp_h\",\n",
        "        \"parse\": \"ip_quad2int\",\n",
        "        \"format\": \"integer\",\n",
        "        \"abnormal\": true,\n",
        "        \"encoding\": \"bit\"\n",
        "      }\n",
        "```\n",
        "* `name`: **required**, key of the field that appeared in the input file.\n",
        "* `parse:` **optional**, name of the parsing function to be applied on the field defined in parse_func.py. Must ensure that the function exist in parse_func.py. Default is `None`. Supported parsing functions:\n",
        "  * second2micro\n",
        "  * ip_quad2int\n",
        "  * modbus_func2code\n",
        "  * syslog_facility2code\n",
        "  * syslog_severity2code\n",
        "* `format`: **optional**, output format of the field (after parsing). Default is `string`. Supported formats:\n",
        "  * string\n",
        "  * float\n",
        "  * integer\n",
        "  * list\n",
        "  * timestamp\n",
        "  * IP\n",
        "* `abnormal`: **optional**, set to `true` if this field needs abnormal handling. Default is `false`.\n",
        "* `encoding`: **required**, the encoding method to be used on the field when training NetShare model. Supported encoding methods:\n",
        "  * bit\n",
        "  * word_proto\n",
        "  * categorical\n",
        "  * list_attributes\n",
        "  * float\n",
        "\n",
        "#### Examples for some special data types\n",
        "eg. list\n",
        "```\n",
        "      {\n",
        "        \"name\": \"DNS__answers\",\n",
        "        \"format\": \"list\",\n",
        "        \"abnormal\": true,\n",
        "        \"encoding\": \"list_values\",\n",
        "        \"names\": {\n",
        "          \"name\": \"categorical\",\n",
        "          \"type\": \"float\",\n",
        "          \"cls\": \"float\",\n",
        "          \"ttl\": \"float\",\n",
        "          \"dlen\": \"float\",\n",
        "          \"address\": \"categorical\"\n",
        "        },\n",
        "        \"delimiter\": \"=\"\n",
        "      }\n",
        "```\n",
        "eg. IP\n",
        "```\n",
        "      {\n",
        "        \"name\": \"IP__src_s\",\n",
        "        \"format\": \"IP\",\n",
        "        \"encoding\": \"bit\",\n",
        "        \"type\": \"IPv4\"\n",
        "      }\n",
        "```\n",
        "eg. timestamp\n",
        "```\n",
        "      {\n",
        "        \"name\": \"ts\",\n",
        "        \"parse\": \"second2micro\",\n",
        "        \"format\": \"timestamp\",\n",
        "        \"encoding\": \"timestamp\"\n",
        "      }\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "1PHcGruQM-xA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to this [link](https://github.com/netsharecmu/NetShare_Summer2023_Internship/blob/modbus_processors/NetShare/examples/syslog/config.json) for whole syslog example config file."
      ],
      "metadata": {
        "id": "qH2Q7x1Eh5Qz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run with NetShare\n",
        "Create a drive.py file\n",
        "```\n",
        "from netshare.driver import Driver\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    driver = Driver(\n",
        "        working_dir_name='syslog',\n",
        "        dataset_file='syslog.pcap',\n",
        "        config_file='config.json',\n",
        "        local_web_port=8060\n",
        "    )\n",
        "    driver.run()\n",
        "```\n",
        "* working_dir_name: change to your working directory name.\n",
        "* dataset_file: change to dataset file name.\n",
        "* config_file: change to the configuration file name.\n",
        "* local_web_port: choose a port number.\n",
        "\n",
        "Run the driver file\n",
        "\n",
        "\n",
        "```\n",
        "python3 driver.py\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "xuQaZZBecyYc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example Usages\n",
        "provide the example dir path\n",
        "*   [Syslog](https://github.com/netsharecmu/NetShare_Summer2023_Internship/tree/main/NetShare/examples/syslog)\n",
        "> A short description of this dataset\n",
        "*   [Modbus](https://github.com/netsharecmu/NetShare_Summer2023_Internship/tree/main/NetShare/examples/modbus)\n",
        "> A short description of this dataset\n",
        "*   [Rahul's Dataset](https://github.com/netsharecmu/NetShare_Summer2023_Internship/tree/main/NetShare/examples/rahul_dataset)\n",
        "> A short description of this dataset\n",
        "*   [Alibaba Microservices Traces](https://github.com/netsharecmu/NetShare_Summer2023_Internship/tree/modbus_processors/NetShare/examples/Alibaba-Cluster-Trace)\n",
        "> The released traces contain the detailed runtime metrics of nearly twenty thousand microservices. They are collected from Alibaba production clusters of over ten thousand bare-metal nodes during twelve hours in 2021.\n",
        "*   [Cyber Security Events](https://github.com/netsharecmu/NetShare_Summer2023_Internship/tree/main/NetShare/examples/Cyber-Security-Events)\n",
        "> This data set represents 58 consecutive days of de-identified event data collected from five sources within Los Alamos National Laboratoryâ€™s corporate, internal computer network.\n",
        "*   [JPM Customer Journey Trace](https://github.com/netsharecmu/NetShare_Summer2023_Internship/tree/main/NetShare/examples/JPM-Customer-Journey)\n",
        "> Customer journey events represent sequences of lower level retail banking clientsâ€™ interactions with the bank. Example types of events include login to a web application, making payments, withdrawing money from ATM machines.  The data was generated by running an AI planning-execution simulator and translating the output planning traces into tabular format.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DFYN5oBVGFoD"
      }
    }
  ]
}